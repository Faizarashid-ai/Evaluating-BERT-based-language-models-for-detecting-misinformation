# Evaluating-BERT-based-language-models-for-detecting-misinformation
 Online misinformation poses a significant challenge due to its rapid spread and limited supervision. To address
 this issue, automated rumour detection techniques are essential for countering the negative impact of false
 information. Previous research primarily focussed on extracting text features, which proved time-consuming and
 less effective. In this study, we contribute substantially to two domains: rumour detection on Twitter and the
 evaluation of text embeddings. We thoroughly analyse rumour detection models and compare the quality of text
 embeddings generated by various fine-tuned BERT-based models. Our findings indicate that our proposed models
 outperform existing techniques. Notably, when we test these models on combined datasets, we observe significant
 performance improvements with larger training and testing data sizes. We conclude that carefully considering the
 dataset, data splitting, and classification techniques is crucial for evaluating solution performance. Additionally,
 we find that differences in the quality of text embeddings between RoBERTa, BERT, and DistilBERT are
 insignificant. This challenges existing assumptions and highlights the need for future research to explore these
 nuances further.
